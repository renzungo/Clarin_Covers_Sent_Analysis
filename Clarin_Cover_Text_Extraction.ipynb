{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Er_ZpO43xa9n"
      },
      "outputs": [],
      "source": [
        "#=====================\n",
        "# 0) ENV & INSTALL\n",
        "# =====================\n",
        "!apt-get -y install tesseract-ocr tesseract-ocr-spa > /dev/null\n",
        "!pip -q install opencv-python-headless pytesseract pandas numpy rapidfuzz pillow tqdm unidecode > /dev/null\n",
        "# Optional (uncomment if you want PaddleOCR fallback)\n",
        "# !pip -q install paddlepaddle==2.6.1 paddleocr==2.7.0.3 > /dev/null\n",
        "\n",
        "import os, sys, json, math, shutil, glob, re, io\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import pytesseract\n",
        "from rapidfuzz import fuzz\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Upgrade to tessdata_best for Spanish (improves accuracy).\n",
        "# Colab tessdata path is typically /usr/share/tesseract-ocr/4.00/tessdata\n",
        "TESSDATA_DIR = \"/usr/share/tesseract-ocr/4.00/tessdata\"\n",
        "os.makedirs(TESSDATA_DIR, exist_ok=True)\n",
        "\n",
        "# Download spa.traineddata (best) if not present\n",
        "if not Path(TESSDATA_DIR, \"spa.traineddata\").exists():\n",
        "    import urllib.request\n",
        "    url = \"https://github.com/tesseract-ocr/tessdata_best/raw/main/spa.traineddata\"\n",
        "    print(\"Downloading tessdata_best spa...\")\n",
        "    urllib.request.urlretrieve(url, str(Path(TESSDATA_DIR, \"spa.traineddata\")))\n",
        "\n",
        "# (optional) English best\n",
        "if not Path(TESSDATA_DIR, \"eng.traineddata\").exists():\n",
        "    import urllib.request\n",
        "    url = \"https://github.com/tesseract-ocr/tessdata_best/raw/main/eng.traineddata\"\n",
        "    print(\"Downloading tessdata_best eng...\")\n",
        "    urllib.request.urlretrieve(url, str(Path(TESSDATA_DIR, \"eng.traineddata\")))\n",
        "\n",
        "os.environ[\"TESSDATA_PREFIX\"] = \"/usr/share/tesseract-ocr/4.00/tessdata\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7B1OdeeByQMI"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# 1) CONFIG\n",
        "# =====================\n",
        "# If using Google Drive, mount and set INPUT_DIR to your folder of covers\n",
        "USE_GOOGLE_DRIVE = False\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Directory containing .jpg covers (change this!)\n",
        "INPUT_DIR = \"/content/covers\"  # e.g., your folder with 600+ images\n",
        "# Where outputs (txt + csv + debug) will go\n",
        "OUTPUT_DIR = \"/content\"\n",
        "\n",
        "# Batch options\n",
        "RECURSIVE = True              # scan subfolders\n",
        "N_WORKERS = 0                 # 0=single-thread (Colab CPU can struggle with cv2 in threads)\n",
        "SAVE_DEBUG_VIS = False        # if True, saves preprocessed and box overlays\n",
        "USE_EAST_DETECTOR = False     # True: detect text boxes first (better layout, slower)\n",
        "EAST_MODEL_URL = \"https://github.com/oyyd/frozendict/releases/download/v0.0.0/frozen_east_text_detection.pb\"  # small mirror; replace if needed\n",
        "\n",
        "# Tesseract options\n",
        "LANGS = \"spa+eng\"\n",
        "DEFAULT_PSM = 6               # for single blocks/paragraphs; 4 for multi-column full page\n",
        "DEFAULT_OEM = 1               # LSTM only\n",
        "MIN_CONF = 58\n",
        "\n",
        "# Optional: PaddleOCR fallback (set to True after installing)\n",
        "USE_PADDLE = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pjCGZqrJQiGH"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# 2) UTILITIES\n",
        "# =====================\n",
        "\n",
        "def imread_unicode(path):\n",
        "    # cv2 doesn't like some unicode paths sometimes — use PIL\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(io.BytesIO(f.read()))\n",
        "        return cv2.cvtColor(np.array(img.convert('RGB')), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "\n",
        "def save_txt(path, text):\n",
        "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        f.write(text)\n",
        "\n",
        "\n",
        "def ensure_dir(p):\n",
        "    Path(p).mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "khvzBuFzQk5_"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# 3) PREPROCESSING\n",
        "# =====================\n",
        "from math import degrees\n",
        "\n",
        "def preprocess_for_ocr(img_bgr, target_long_edge=2600):\n",
        "    h, w = img_bgr.shape[:2]\n",
        "    scale = target_long_edge / max(h, w)\n",
        "    if scale != 1.0:\n",
        "        img_bgr = cv2.resize(img_bgr, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    # Denoise JPEG artifacts\n",
        "    img_bgr = cv2.fastNlMeansDenoisingColored(img_bgr, None, 7, 7, 7, 21)\n",
        "\n",
        "    # LAB -> L channel + CLAHE\n",
        "    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n",
        "    L, A, B = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
        "    Lc = clahe.apply(L)\n",
        "    gray = Lc\n",
        "\n",
        "    # Adaptive threshold\n",
        "    bin_img = cv2.adaptiveThreshold(gray, 255,\n",
        "                                    cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                    cv2.THRESH_BINARY, 35, 11)\n",
        "\n",
        "    # Unsharp mask\n",
        "    blur = cv2.GaussianBlur(bin_img, (0,0), 1.0)\n",
        "    sharp = cv2.addWeighted(bin_img, 1.5, blur, -0.5, 0)\n",
        "\n",
        "    # Deskew via Hough lines\n",
        "    edges = cv2.Canny(sharp, 80, 160)\n",
        "    lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=150)\n",
        "    angle = 0.0\n",
        "    if lines is not None:\n",
        "        angles = []\n",
        "        for rho, theta in lines[:,0]:\n",
        "            a = degrees(theta)\n",
        "            if a < 45 or a > 135:  # horizontal-ish\n",
        "                ang = a-180 if a>90 else a\n",
        "                angles.append(ang)\n",
        "        if len(angles):\n",
        "            angle = float(np.median(angles))\n",
        "\n",
        "    if abs(angle) > 0.5:\n",
        "        (h2, w2) = sharp.shape[:2]\n",
        "        M = cv2.getRotationMatrix2D((w2//2, h2//2), angle, 1.0)\n",
        "        sharp = cv2.warpAffine(sharp, M, (w2, h2), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
        "\n",
        "    return sharp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mwlziL4uQo88"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# 4) (OPTIONAL) TEXT DETECTION WITH EAST\n",
        "# =====================\n",
        "EAST_PATH = \"/content/east_text_detection.pb\"\n",
        "\n",
        "def ensure_east_model():\n",
        "    if not Path(EAST_PATH).exists():\n",
        "        import urllib.request\n",
        "        print(\"Downloading EAST model (1.4MB)...\")\n",
        "        urllib.request.urlretrieve(EAST_MODEL_URL, EAST_PATH)\n",
        "\n",
        "\n",
        "def detect_text_boxes_east(image_bin, conf=0.55, nms=0.35):\n",
        "    # image_bin is single-channel; EAST expects 3-channel\n",
        "    H, W = image_bin.shape[:2]\n",
        "    image = cv2.cvtColor(image_bin, cv2.COLOR_GRAY2BGR)\n",
        "    net = cv2.dnn.readNet(EAST_PATH)\n",
        "    newW, newH = (W//32)*32, (H//32)*32\n",
        "    rW, rH = W / float(newW), H / float(newH)\n",
        "    blob = cv2.dnn.blobFromImage(image, 1.0, (newW, newH), (123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    (scores, geometry) = net.forward([\"feature_fusion/Conv_7/Sigmoid\", \"feature_fusion/concat_3\"])\n",
        "\n",
        "    # decode\n",
        "    numRows, numCols = scores.shape[2:4]\n",
        "    rects, confidences = [], []\n",
        "    for y in range(numRows):\n",
        "        scoresData = scores[0,0,y]\n",
        "        xData0 = geometry[0,0,y]\n",
        "        xData1 = geometry[0,1,y]\n",
        "        xData2 = geometry[0,2,y]\n",
        "        xData3 = geometry[0,3,y]\n",
        "        angles = geometry[0,4,y]\n",
        "        for x in range(numCols):\n",
        "            if scoresData[x] < conf:\n",
        "                continue\n",
        "            offsetX, offsetY = x*4.0, y*4.0\n",
        "            angle = angles[x]\n",
        "            cos, sin = np.cos(angle), np.sin(angle)\n",
        "            h = xData0[x] + xData2[x]\n",
        "            w = xData1[x] + xData3[x]\n",
        "            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
        "            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
        "            startX = int(endX - w)\n",
        "            startY = int(endY - h)\n",
        "            # scale back up\n",
        "            startX = int(startX * rW); startY = int(startY * rH)\n",
        "            endX = int(endX * rW); endY = int(endY * rH)\n",
        "            rects.append((startX, startY, endX, endY))\n",
        "            confidences.append(float(scoresData[x]))\n",
        "\n",
        "    if not rects:\n",
        "        return []\n",
        "\n",
        "    boxes = cv2.dnn.NMSBoxes(\n",
        "        bboxes=[(x, y, ex-x, ey-y) for (x,y,ex,ey) in rects],\n",
        "        scores=confidences, score_threshold=conf, nms_threshold=nms)\n",
        "\n",
        "    out = []\n",
        "    if len(boxes) > 0:\n",
        "        for i in boxes.flatten():\n",
        "            x,y, w,h = int(rects[i][0]), int(rects[i][1]), int(rects[i][2]-rects[i][0]), int(rects[i][3]-rects[i][1])\n",
        "            pad = 4\n",
        "            out.append((max(0,x-pad), max(0,y-pad), min(W, x+w+pad), min(H, y+h+pad)))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FtR1Amj_Qxk-"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# 5) OCR + CLEANUP\n",
        "# =====================\n",
        "\n",
        "def ocr_tesseract(img_bin, lang=LANGS, psm=DEFAULT_PSM, oem=DEFAULT_OEM, min_conf=MIN_CONF):\n",
        "    cfg = f\"--oem {oem} --psm {psm} -c preserve_interword_spaces=1\"\n",
        "    df = pytesseract.image_to_data(img_bin, lang=lang, config=cfg, output_type=pytesseract.Output.DATAFRAME)\n",
        "    df = df.dropna()\n",
        "    if 'conf' in df:\n",
        "        df = df[df['conf'].astype(int) >= min_conf]\n",
        "    lines = []\n",
        "    for (page, block, par, line), g in df.groupby(['page_num','block_num','par_num','line_num']):\n",
        "        words = g.sort_values('left')['text'].astype(str).tolist()\n",
        "        line_text = ' '.join([w for w in words if w.strip()])\n",
        "        if line_text.strip():\n",
        "            lines.append(line_text.strip())\n",
        "    return '\\n'.join(lines), df\n",
        "\n",
        "\n",
        "def cleanup_text(text: str) -> str:\n",
        "    t = text\n",
        "    t = re.sub(r'[ \\t]+', ' ', t)\n",
        "    t = re.sub(r'(\\w)-\\n(\\w)', r'\\1\\2', t)  # join hyphenated at EOL\n",
        "    t = t.replace('“','\"').replace('”','\"').replace('’',\"'\").replace('‘',\"'\")\n",
        "    t = re.sub(r'(?<![.!?])\\n(?!\\n)', ' ', t)  # merge single breaks within sentences\n",
        "    t = re.sub(r'\\n{3,}', '\\n\\n', t)\n",
        "    # token fixes for ALL-CAPS with digits\n",
        "    def fix_token(tok):\n",
        "        if tok.isupper() and any(c.isdigit() for c in tok):\n",
        "            tok = tok.replace('0','O').replace('1','I').replace('5','S')\n",
        "        return tok\n",
        "    t = ' '.join(fix_token(tok) for tok in t.split())\n",
        "    return t.strip()\n",
        "\n",
        "# Optional PaddleOCR fallback\n",
        "PADDLE_OCR = None\n",
        "if USE_PADDLE:\n",
        "    from paddleocr import PaddleOCR\n",
        "    PADDLE_OCR = PaddleOCR(use_angle_cls=True, lang='es', show_log=False)\n",
        "\n",
        "\n",
        "def ocr_paddle(img_bgr):\n",
        "    # Paddle works better on color/gray image, not binary only\n",
        "    result = PADDLE_OCR.ocr(img_bgr, cls=True)\n",
        "    lines = []\n",
        "    for res in result:\n",
        "        for box, (txt, prob) in res:\n",
        "            if prob >= 0.45:\n",
        "                lines.append(txt)\n",
        "    return '\\n'.join(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0_yqGNoMQzNH"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# 6) IMAGE → TEXT PIPELINE (ONE FILE)\n",
        "# =====================\n",
        "\n",
        "def process_one(image_path, out_dir=OUTPUT_DIR, save_debug=SAVE_DEBUG_VIS):\n",
        "    try:\n",
        "        img_bgr = imread_unicode(image_path)\n",
        "    except Exception as e:\n",
        "        return {\"image\": image_path, \"ok\": False, \"error\": f\"read_error: {e}\"}\n",
        "\n",
        "    pre = preprocess_for_ocr(img_bgr)\n",
        "\n",
        "    all_text = []\n",
        "    box_data = []\n",
        "\n",
        "    if USE_EAST_DETECTOR:\n",
        "        ensure_east_model()\n",
        "        boxes = detect_text_boxes_east(pre)\n",
        "        if not boxes:\n",
        "            # fallback to whole-page OCR\n",
        "            text, df = ocr_tesseract(pre, psm=4)\n",
        "            all_text.append(text)\n",
        "            if df is not None and len(df):\n",
        "                for _, r in df.iterrows():\n",
        "                    box_data.append({\"left\": int(r.get('left',0)), \"top\": int(r.get('top',0)),\n",
        "                                     \"width\": int(r.get('width',0)), \"height\": int(r.get('height',0)),\n",
        "                                     \"conf\": float(r.get('conf',0)), \"text\": r.get('text','')})\n",
        "        else:\n",
        "            # Sort boxes top-to-bottom, then left-to-right\n",
        "            boxes = sorted(boxes, key=lambda b: (b[1], b[0]))\n",
        "            for (x1,y1,x2,y2) in boxes:\n",
        "                roi = pre[y1:y2, x1:x2]\n",
        "                # choose PSM by aspect ratio/size\n",
        "                h, w = roi.shape[:2]\n",
        "                psm = 6 if w/h > 1.2 else 7  # heuristic: wide block likely a line/paragraph\n",
        "                text, df = ocr_tesseract(roi, psm=psm)\n",
        "                if text.strip():\n",
        "                    all_text.append(text)\n",
        "                if df is not None and len(df):\n",
        "                    for _, r in df.iterrows():\n",
        "                        box_data.append({\"left\": int(x1+int(r.get('left',0))),\n",
        "                                         \"top\": int(y1+int(r.get('top',0))),\n",
        "                                         \"width\": int(r.get('width',0)),\n",
        "                                         \"height\": int(r.get('height',0)),\n",
        "                                         \"conf\": float(r.get('conf',0)),\n",
        "                                         \"text\": r.get('text','')})\n",
        "    else:\n",
        "        text, df = ocr_tesseract(pre, psm=4)  # full page, multi-column\n",
        "        all_text.append(text)\n",
        "        if df is not None and len(df):\n",
        "            for _, r in df.iterrows():\n",
        "                box_data.append({\"left\": int(r.get('left',0)), \"top\": int(r.get('top',0)),\n",
        "                                 \"width\": int(r.get('width',0)), \"height\": int(r.get('height',0)),\n",
        "                                 \"conf\": float(r.get('conf',0)), \"text\": r.get('text','')})\n",
        "\n",
        "    raw_text = '\\n'.join([t for t in all_text if t.strip()])\n",
        "    cleaned = cleanup_text(raw_text)\n",
        "\n",
        "    # Optional Paddle fallback if very short and Paddle is enabled\n",
        "    if USE_PADDLE and len(cleaned) < 30:\n",
        "        paddle_txt = ocr_paddle(img_bgr)\n",
        "        if len(paddle_txt) > len(cleaned):\n",
        "            cleaned = cleanup_text(paddle_txt)\n",
        "\n",
        "    rel = os.path.relpath(image_path, INPUT_DIR)\n",
        "    stem = Path(rel).with_suffix(\"\")\n",
        "\n",
        "    txt_out = Path(out_dir, \"txt\", f\"{stem}.txt\")\n",
        "    json_out = Path(out_dir, \"json\", f\"{stem}.json\")\n",
        "    dbg_dir  = Path(out_dir, \"debug\")\n",
        "\n",
        "    save_txt(txt_out, cleaned)\n",
        "\n",
        "    # Save JSON with boxes + stats\n",
        "    rec = {\n",
        "        \"image\": image_path,\n",
        "        \"text_path\": str(txt_out),\n",
        "        \"n_chars\": len(cleaned),\n",
        "        \"n_lines\": cleaned.count('\\n') + 1 if cleaned else 0,\n",
        "        \"use_east\": USE_EAST_DETECTOR,\n",
        "        \"lang\": LANGS,\n",
        "        \"box_data\": box_data[:5000]  # avoid overly large JSONs\n",
        "    }\n",
        "    Path(json_out).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(json_out, 'w', encoding='utf-8') as f:\n",
        "        json.dump(rec, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Save debug images\n",
        "    if save_debug:\n",
        "        ensure_dir(dbg_dir)\n",
        "        pre_path = Path(dbg_dir, f\"{stem}_pre.png\")\n",
        "        Path(pre_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "        cv2.imwrite(str(pre_path), pre)\n",
        "        if USE_EAST_DETECTOR:\n",
        "            vis = cv2.cvtColor(pre, cv2.COLOR_GRAY2BGR)\n",
        "            for b in detect_text_boxes_east(pre):\n",
        "                x1,y1,x2,y2 = b\n",
        "                cv2.rectangle(vis, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "            cv2.imwrite(str(Path(dbg_dir, f\"{stem}_boxes.png\")), vis)\n",
        "\n",
        "    return {\"image\": image_path, \"ok\": True, \"text_path\": str(txt_out), \"n_chars\": len(cleaned)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "m4zekk_YQ6nV"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# 7) BATCH RUNNER\n",
        "# =====================\n",
        "\n",
        "def list_images(root, recursive=True):\n",
        "    exts = {'.jpg','.jpeg','.png','.webp','.tif','.tiff'}\n",
        "    files = []\n",
        "    root = Path(root)\n",
        "    if recursive:\n",
        "        for p in root.rglob('*'):\n",
        "            if p.suffix.lower() in exts:\n",
        "                files.append(str(p))\n",
        "    else:\n",
        "        for p in root.glob('*'):\n",
        "            if p.suffix.lower() in exts:\n",
        "                files.append(str(p))\n",
        "    files.sort()\n",
        "    return files\n",
        "\n",
        "\n",
        "def run_batch():\n",
        "    ensure_dir(OUTPUT_DIR)\n",
        "    images = list_images(INPUT_DIR, RECURSIVE)\n",
        "    print(f\"Found {len(images)} images\")\n",
        "\n",
        "    rows = []\n",
        "    for img in tqdm(images):\n",
        "        try:\n",
        "            res = process_one(img)\n",
        "            rows.append(res)\n",
        "        except Exception as e:\n",
        "            rows.append({\"image\": img, \"ok\": False, \"error\": str(e)})\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    csv_path = Path(OUTPUT_DIR, \"batch_summary.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(\"Saved:\", csv_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aOf4jwf_Q94O"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# 8) QUICK TEST (OPTIONAL): download one sample cover and run\n",
        "# =====================\n",
        "TEST_ONE = False\n",
        "if TEST_ONE:\n",
        "    import urllib.request\n",
        "    url = \"https://tapas.clarin.com/tapa/2025/08/06/20250806_thumb.jpg\"\n",
        "    Path('/content/sample_data').mkdir(exist_ok=True, parents=True)\n",
        "    local = \"/content/sample_data/clarin_20250806.jpg\"\n",
        "    urllib.request.urlretrieve(url, local)\n",
        "    INPUT_DIR = \"/content/sample_data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "3dc3aba1c09c45aca64fbfba1b7f7614",
            "abbedeffb7274c2cb1b3195edd1fcbeb",
            "f65dcfa868894d5fbbb6286bdae8b285",
            "3d07afe83bac4a60a61d3f4350bd340e",
            "baba92640c9d4e9fa292f12dac63b9cf",
            "ee66293000c6445399c7736d1e9ee634",
            "4862ca78a9214943b4eeec4d168c8965",
            "2fbe51fdf640469184c790d8dd15ccb3",
            "9c4564d19e3c4588a6c8d4cc96b8fb47",
            "0946689ad1df4ccd9f8e1b2431256408",
            "0edf64bffd924fdf835e82442af4591d"
          ]
        },
        "id": "ltARVoRmRCPK",
        "outputId": "eace8026-c817-4fa8-f1c3-dd25d793f4d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3dc3aba1c09c45aca64fbfba1b7f7614"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/batch_summary.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# =====================\n",
        "# 9) GO — run the batch\n",
        "# =====================\n",
        "run_batch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3dc3aba1c09c45aca64fbfba1b7f7614": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_abbedeffb7274c2cb1b3195edd1fcbeb",
              "IPY_MODEL_f65dcfa868894d5fbbb6286bdae8b285",
              "IPY_MODEL_3d07afe83bac4a60a61d3f4350bd340e"
            ],
            "layout": "IPY_MODEL_baba92640c9d4e9fa292f12dac63b9cf"
          }
        },
        "abbedeffb7274c2cb1b3195edd1fcbeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee66293000c6445399c7736d1e9ee634",
            "placeholder": "​",
            "style": "IPY_MODEL_4862ca78a9214943b4eeec4d168c8965",
            "value": ""
          }
        },
        "f65dcfa868894d5fbbb6286bdae8b285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fbe51fdf640469184c790d8dd15ccb3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c4564d19e3c4588a6c8d4cc96b8fb47",
            "value": 0
          }
        },
        "3d07afe83bac4a60a61d3f4350bd340e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0946689ad1df4ccd9f8e1b2431256408",
            "placeholder": "​",
            "style": "IPY_MODEL_0edf64bffd924fdf835e82442af4591d",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "baba92640c9d4e9fa292f12dac63b9cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee66293000c6445399c7736d1e9ee634": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4862ca78a9214943b4eeec4d168c8965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fbe51fdf640469184c790d8dd15ccb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "9c4564d19e3c4588a6c8d4cc96b8fb47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0946689ad1df4ccd9f8e1b2431256408": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0edf64bffd924fdf835e82442af4591d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}