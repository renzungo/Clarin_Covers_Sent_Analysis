{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renzungo/Clarin_Covers_Sent_Analysis/blob/main/Clarin_Cover_Text_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Er_ZpO43xa9n"
      },
      "outputs": [],
      "source": [
        "#=====================\n",
        "# 0) ENV & INSTALL\n",
        "# =====================\n",
        "!apt-get -y install tesseract-ocr tesseract-ocr-spa > /dev/null\n",
        "!pip -q install opencv-python-headless pytesseract pandas numpy rapidfuzz pillow tqdm unidecode > /dev/null\n",
        "# Optional (uncomment if you want PaddleOCR fallback)\n",
        "# !pip -q install paddlepaddle==2.6.1 paddleocr==2.7.0.3 > /dev/null\n",
        "\n",
        "import os, sys, json, math, shutil, glob, re, io\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import pytesseract\n",
        "from rapidfuzz import fuzz\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Upgrade to tessdata_best for Spanish (improves accuracy).\n",
        "# Colab tessdata path is typically /usr/share/tesseract-ocr/4.00/tessdata\n",
        "TESSDATA_DIR = \"/usr/share/tesseract-ocr/4.00/tessdata\"\n",
        "os.makedirs(TESSDATA_DIR, exist_ok=True)\n",
        "\n",
        "# Download spa.traineddata (best) if not present\n",
        "if not Path(TESSDATA_DIR, \"spa.traineddata\").exists():\n",
        "    import urllib.request\n",
        "    url = \"https://github.com/tesseract-ocr/tessdata_best/raw/main/spa.traineddata\"\n",
        "    print(\"Downloading tessdata_best spa...\")\n",
        "    urllib.request.urlretrieve(url, str(Path(TESSDATA_DIR, \"spa.traineddata\")))\n",
        "\n",
        "# (optional) English best\n",
        "if not Path(TESSDATA_DIR, \"eng.traineddata\").exists():\n",
        "    import urllib.request\n",
        "    url = \"https://github.com/tesseract-ocr/tessdata_best/raw/main/eng.traineddata\"\n",
        "    print(\"Downloading tessdata_best eng...\")\n",
        "    urllib.request.urlretrieve(url, str(Path(TESSDATA_DIR, \"eng.traineddata\")))\n",
        "\n",
        "os.environ[\"TESSDATA_PREFIX\"] = \"/usr/share/tesseract-ocr/4.00/tessdata\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7B1OdeeByQMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79e30b26-2da2-4e7c-8a29-72ee93f958bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# =====================\n",
        "# 1) CONFIG\n",
        "# =====================\n",
        "# If using Google Drive, mount and set INPUT_DIR to your folder of covers\n",
        "USE_GOOGLE_DRIVE = True\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Directory containing .jpg covers (change this!)\n",
        "INPUT_DIR = \"/content/drive/MyDrive/Data Justicialista/Clarin Cover Sentiment Analysis/Covers\" # e.g., your folder with 600+ images\n",
        "# Where outputs (txt + csv + debug) will go\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Data Justicialista/Clarin Cover Sentiment Analysis/OCR_Out\"\n",
        "\n",
        "\n",
        "# Batch options\n",
        "RECURSIVE = True              # scan subfolders\n",
        "N_WORKERS = 0                 # 0=single-thread (Colab CPU can struggle with cv2 in threads)\n",
        "SAVE_DEBUG_VIS = False        # if True, saves preprocessed and box overlays\n",
        "USE_EAST_DETECTOR = False     # True: detect text boxes first (better layout, slower)\n",
        "EAST_MODEL_URL = \"https://github.com/oyyd/frozendict/releases/download/v0.0.0/frozen_east_text_detection.pb\"  # small mirror; replace if needed\n",
        "\n",
        "# Tesseract options\n",
        "LANGS = \"spa+eng\"\n",
        "DEFAULT_PSM = 6               # for single blocks/paragraphs; 4 for multi-column full page\n",
        "DEFAULT_OEM = 1               # LSTM only\n",
        "MIN_CONF = 58\n",
        "\n",
        "# Optional: PaddleOCR fallback (set to True after installing)\n",
        "USE_PADDLE = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pjCGZqrJQiGH"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# 2) UTILITIES\n",
        "# =====================\n",
        "\n",
        "def imread_unicode(path):\n",
        "    # cv2 doesn't like some unicode paths sometimes — use PIL\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(io.BytesIO(f.read()))\n",
        "        return cv2.cvtColor(np.array(img.convert('RGB')), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "\n",
        "def save_txt(path, text):\n",
        "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        f.write(text)\n",
        "\n",
        "\n",
        "def ensure_dir(p):\n",
        "    Path(p).mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "khvzBuFzQk5_"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# 3) PREPROCESSING\n",
        "# =====================\n",
        "from math import degrees\n",
        "\n",
        "def preprocess_for_ocr(img_bgr, target_long_edge=2600):\n",
        "    h, w = img_bgr.shape[:2]\n",
        "    scale = target_long_edge / max(h, w)\n",
        "    if scale != 1.0:\n",
        "        img_bgr = cv2.resize(img_bgr, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    # Denoise JPEG artifacts\n",
        "    img_bgr = cv2.fastNlMeansDenoisingColored(img_bgr, None, 7, 7, 7, 21)\n",
        "\n",
        "    # LAB -> L channel + CLAHE\n",
        "    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n",
        "    L, A, B = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
        "    Lc = clahe.apply(L)\n",
        "    gray = Lc\n",
        "\n",
        "    # Adaptive threshold\n",
        "    bin_img = cv2.adaptiveThreshold(gray, 255,\n",
        "                                    cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                    cv2.THRESH_BINARY, 35, 11)\n",
        "\n",
        "    # Unsharp mask\n",
        "    blur = cv2.GaussianBlur(bin_img, (0,0), 1.0)\n",
        "    sharp = cv2.addWeighted(bin_img, 1.5, blur, -0.5, 0)\n",
        "\n",
        "    # Deskew via Hough lines\n",
        "    edges = cv2.Canny(sharp, 80, 160)\n",
        "    lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=150)\n",
        "    angle = 0.0\n",
        "    if lines is not None:\n",
        "        angles = []\n",
        "        for rho, theta in lines[:,0]:\n",
        "            a = degrees(theta)\n",
        "            if a < 45 or a > 135:  # horizontal-ish\n",
        "                ang = a-180 if a>90 else a\n",
        "                angles.append(ang)\n",
        "        if len(angles):\n",
        "            angle = float(np.median(angles))\n",
        "\n",
        "    if abs(angle) > 0.5:\n",
        "        (h2, w2) = sharp.shape[:2]\n",
        "        M = cv2.getRotationMatrix2D((w2//2, h2//2), angle, 1.0)\n",
        "        sharp = cv2.warpAffine(sharp, M, (w2, h2), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
        "\n",
        "    return sharp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mwlziL4uQo88"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# 4) (OPTIONAL) TEXT DETECTION WITH EAST\n",
        "# =====================\n",
        "EAST_PATH = \"/content/east_text_detection.pb\"\n",
        "\n",
        "def ensure_east_model():\n",
        "    if not Path(EAST_PATH).exists():\n",
        "        import urllib.request\n",
        "        print(\"Downloading EAST model (1.4MB)...\")\n",
        "        urllib.request.urlretrieve(EAST_MODEL_URL, EAST_PATH)\n",
        "\n",
        "\n",
        "def detect_text_boxes_east(image_bin, conf=0.55, nms=0.35):\n",
        "    # image_bin is single-channel; EAST expects 3-channel\n",
        "    H, W = image_bin.shape[:2]\n",
        "    image = cv2.cvtColor(image_bin, cv2.COLOR_GRAY2BGR)\n",
        "    net = cv2.dnn.readNet(EAST_PATH)\n",
        "    newW, newH = (W//32)*32, (H//32)*32\n",
        "    rW, rH = W / float(newW), H / float(newH)\n",
        "    blob = cv2.dnn.blobFromImage(image, 1.0, (newW, newH), (123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    (scores, geometry) = net.forward([\"feature_fusion/Conv_7/Sigmoid\", \"feature_fusion/concat_3\"])\n",
        "\n",
        "    # decode\n",
        "    numRows, numCols = scores.shape[2:4]\n",
        "    rects, confidences = [], []\n",
        "    for y in range(numRows):\n",
        "        scoresData = scores[0,0,y]\n",
        "        xData0 = geometry[0,0,y]\n",
        "        xData1 = geometry[0,1,y]\n",
        "        xData2 = geometry[0,2,y]\n",
        "        xData3 = geometry[0,3,y]\n",
        "        angles = geometry[0,4,y]\n",
        "        for x in range(numCols):\n",
        "            if scoresData[x] < conf:\n",
        "                continue\n",
        "            offsetX, offsetY = x*4.0, y*4.0\n",
        "            angle = angles[x]\n",
        "            cos, sin = np.cos(angle), np.sin(angle)\n",
        "            h = xData0[x] + xData2[x]\n",
        "            w = xData1[x] + xData3[x]\n",
        "            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
        "            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
        "            startX = int(endX - w)\n",
        "            startY = int(endY - h)\n",
        "            # scale back up\n",
        "            startX = int(startX * rW); startY = int(startY * rH)\n",
        "            endX = int(endX * rW); endY = int(endY * rH)\n",
        "            rects.append((startX, startY, endX, endY))\n",
        "            confidences.append(float(scoresData[x]))\n",
        "\n",
        "    if not rects:\n",
        "        return []\n",
        "\n",
        "    boxes = cv2.dnn.NMSBoxes(\n",
        "        bboxes=[(x, y, ex-x, ey-y) for (x,y,ex,ey) in rects],\n",
        "        scores=confidences, score_threshold=conf, nms_threshold=nms)\n",
        "\n",
        "    out = []\n",
        "    if len(boxes) > 0:\n",
        "        for i in boxes.flatten():\n",
        "            x,y, w,h = int(rects[i][0]), int(rects[i][1]), int(rects[i][2]-rects[i][0]), int(rects[i][3]-rects[i][1])\n",
        "            pad = 4\n",
        "            out.append((max(0,x-pad), max(0,y-pad), min(W, x+w+pad), min(H, y+h+pad)))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FtR1Amj_Qxk-"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# 5) OCR + CLEANUP\n",
        "# =====================\n",
        "\n",
        "def ocr_tesseract(img_bin, lang=LANGS, psm=DEFAULT_PSM, oem=DEFAULT_OEM, min_conf=MIN_CONF):\n",
        "    cfg = f\"--oem {oem} --psm {psm} -c preserve_interword_spaces=1\"\n",
        "    df = pytesseract.image_to_data(img_bin, lang=lang, config=cfg, output_type=pytesseract.Output.DATAFRAME)\n",
        "    df = df.dropna()\n",
        "    if 'conf' in df:\n",
        "        df = df[df['conf'].astype(int) >= min_conf]\n",
        "    lines = []\n",
        "    for (page, block, par, line), g in df.groupby(['page_num','block_num','par_num','line_num']):\n",
        "        words = g.sort_values('left')['text'].astype(str).tolist()\n",
        "        line_text = ' '.join([w for w in words if w.strip()])\n",
        "        if line_text.strip():\n",
        "            lines.append(line_text.strip())\n",
        "    return '\\n'.join(lines), df\n",
        "\n",
        "\n",
        "def cleanup_text(text: str) -> str:\n",
        "    t = text\n",
        "    t = re.sub(r'[ \\t]+', ' ', t)\n",
        "    t = re.sub(r'(\\w)-\\n(\\w)', r'\\1\\2', t)  # join hyphenated at EOL\n",
        "    t = t.replace('“','\"').replace('”','\"').replace('’',\"'\").replace('‘',\"'\")\n",
        "    t = re.sub(r'(?<![.!?])\\n(?!\\n)', ' ', t)  # merge single breaks within sentences\n",
        "    t = re.sub(r'\\n{3,}', '\\n\\n', t)\n",
        "    # token fixes for ALL-CAPS with digits\n",
        "    def fix_token(tok):\n",
        "        if tok.isupper() and any(c.isdigit() for c in tok):\n",
        "            tok = tok.replace('0','O').replace('1','I').replace('5','S')\n",
        "        return tok\n",
        "    t = ' '.join(fix_token(tok) for tok in t.split())\n",
        "    return t.strip()\n",
        "\n",
        "# Optional PaddleOCR fallback\n",
        "PADDLE_OCR = None\n",
        "if USE_PADDLE:\n",
        "    from paddleocr import PaddleOCR\n",
        "    PADDLE_OCR = PaddleOCR(use_angle_cls=True, lang='es', show_log=False)\n",
        "\n",
        "\n",
        "def ocr_paddle(img_bgr):\n",
        "    # Paddle works better on color/gray image, not binary only\n",
        "    result = PADDLE_OCR.ocr(img_bgr, cls=True)\n",
        "    lines = []\n",
        "    for res in result:\n",
        "        for box, (txt, prob) in res:\n",
        "            if prob >= 0.45:\n",
        "                lines.append(txt)\n",
        "    return '\\n'.join(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "m4zekk_YQ6nV"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# 7) BATCH RUNNER\n",
        "# =====================\n",
        "\n",
        "def list_images(root, recursive=True):\n",
        "    exts = {'.jpg','.jpeg','.png','.webp','.tif','.tiff'}\n",
        "    files = []\n",
        "    root = Path(root)\n",
        "    if recursive:\n",
        "        for p in root.rglob('*'):\n",
        "            if p.suffix.lower() in exts:\n",
        "                files.append(str(p))\n",
        "    else:\n",
        "        for p in root.glob('*'):\n",
        "            if p.suffix.lower() in exts:\n",
        "                files.append(str(p))\n",
        "    files.sort()\n",
        "    return files\n",
        "\n",
        "\n",
        "def run_batch():\n",
        "    ensure_dir(OUTPUT_DIR)\n",
        "    images = list_images(INPUT_DIR, RECURSIVE)\n",
        "    print(f\"Found {len(images)} images\")\n",
        "\n",
        "    rows = []\n",
        "    for img in tqdm(images):\n",
        "        try:\n",
        "            res = process_one(img)\n",
        "            rows.append(res)\n",
        "        except Exception as e:\n",
        "            rows.append({\"image\": img, \"ok\": False, \"error\": str(e)})\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    csv_path = Path(OUTPUT_DIR, \"batch_summary.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(\"Saved:\", csv_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0_yqGNoMQzNH"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# 6) IMAGE → TEXT PIPELINE (ONE FILE)\n",
        "# =====================\n",
        "\n",
        "def process_one(image_path, out_dir=OUTPUT_DIR, save_debug=SAVE_DEBUG_VIS):\n",
        "    try:\n",
        "        img_bgr = imread_unicode(image_path)\n",
        "    except Exception as e:\n",
        "        return {\"image\": image_path, \"ok\": False, \"error\": f\"read_error: {e}\"}\n",
        "\n",
        "    pre = preprocess_for_ocr(img_bgr)\n",
        "\n",
        "    all_text = []\n",
        "    box_data = []\n",
        "\n",
        "    if USE_EAST_DETECTOR:\n",
        "        ensure_east_model()\n",
        "        boxes = detect_text_boxes_east(pre)\n",
        "        if not boxes:\n",
        "            # fallback to whole-page OCR\n",
        "            text, df = ocr_tesseract(pre, psm=4)\n",
        "            all_text.append(text)\n",
        "            if df is not None and len(df):\n",
        "                for _, r in df.iterrows():\n",
        "                    box_data.append({\"left\": int(r.get('left',0)), \"top\": int(r.get('top',0)),\n",
        "                                     \"width\": int(r.get('width',0)), \"height\": int(r.get('height',0)),\n",
        "                                     \"conf\": float(r.get('conf',0)), \"text\": r.get('text','')})\n",
        "        else:\n",
        "            # Sort boxes top-to-bottom, then left-to-right\n",
        "            boxes = sorted(boxes, key=lambda b: (b[1], b[0]))\n",
        "            for (x1,y1,x2,y2) in boxes:\n",
        "                roi = pre[y1:y2, x1:x2]\n",
        "                # choose PSM by aspect ratio/size\n",
        "                h, w = roi.shape[:2]\n",
        "                psm = 6 if w/h > 1.2 else 7  # heuristic: wide block likely a line/paragraph\n",
        "                text, df = ocr_tesseract(roi, psm=psm)\n",
        "                if text.strip():\n",
        "                    all_text.append(text)\n",
        "                if df is not None and len(df):\n",
        "                    for _, r in df.iterrows():\n",
        "                        box_data.append({\"left\": int(x1+int(r.get('left',0))),\n",
        "                                         \"top\": int(y1+int(r.get('top',0))),\n",
        "                                         \"width\": int(r.get('width',0)),\n",
        "                                         \"height\": int(r.get('height',0)),\n",
        "                                         \"conf\": float(r.get('conf',0)),\n",
        "                                         \"text\": r.get('text','')})\n",
        "    else:\n",
        "        text, df = ocr_tesseract(pre, psm=4)  # full page, multi-column\n",
        "        all_text.append(text)\n",
        "        if df is not None and len(df):\n",
        "            for _, r in df.iterrows():\n",
        "                box_data.append({\"left\": int(r.get('left',0)), \"top\": int(r.get('top',0)),\n",
        "                                 \"width\": int(r.get('width',0)), \"height\": int(r.get('height',0)),\n",
        "                                 \"conf\": float(r.get('conf',0)), \"text\": r.get('text','')})\n",
        "\n",
        "    raw_text = '\\n'.join([t for t in all_text if t.strip()])\n",
        "    cleaned = cleanup_text(raw_text)\n",
        "\n",
        "    # Optional Paddle fallback if very short and Paddle is enabled\n",
        "    if USE_PADDLE and len(cleaned) < 30:\n",
        "        paddle_txt = ocr_paddle(img_bgr)\n",
        "        if len(paddle_txt) > len(cleaned):\n",
        "            cleaned = cleanup_text(paddle_txt)\n",
        "\n",
        "    rel = os.path.relpath(image_path, INPUT_DIR)\n",
        "    stem = Path(rel).with_suffix(\"\")\n",
        "\n",
        "    txt_out = Path(out_dir, \"txt\", f\"{stem}.txt\")\n",
        "    json_out = Path(out_dir, \"json\", f\"{stem}.json\")\n",
        "    dbg_dir  = Path(out_dir, \"debug\")\n",
        "\n",
        "    save_txt(txt_out, cleaned)\n",
        "\n",
        "    # Save JSON with boxes + stats\n",
        "    rec = {\n",
        "        \"image\": image_path,\n",
        "        \"text_path\": str(txt_out),\n",
        "        \"n_chars\": len(cleaned),\n",
        "        \"n_lines\": cleaned.count('\\n') + 1 if cleaned else 0,\n",
        "        \"use_east\": USE_EAST_DETECTOR,\n",
        "        \"lang\": LANGS,\n",
        "        \"box_data\": box_data[:5000]  # avoid overly large JSONs\n",
        "    }\n",
        "    Path(json_out).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(json_out, 'w', encoding='utf-8') as f:\n",
        "        json.dump(rec, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Save debug images\n",
        "    if save_debug:\n",
        "        ensure_dir(dbg_dir)\n",
        "        pre_path = Path(dbg_dir, f\"{stem}_pre.png\")\n",
        "        Path(pre_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "        cv2.imwrite(str(pre_path), pre)\n",
        "        if USE_EAST_DETECTOR:\n",
        "            vis = cv2.cvtColor(pre, cv2.COLOR_GRAY2BGR)\n",
        "            for b in detect_text_boxes_east(pre):\n",
        "                x1,y1,x2,y2 = b\n",
        "                cv2.rectangle(vis, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "            cv2.imwrite(str(Path(dbg_dir, f\"{stem}_boxes.png\")), vis)\n",
        "\n",
        "    return {\"image\": image_path, \"ok\": True, \"text_path\": str(txt_out), \"n_chars\": len(cleaned)}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= RESUMABLE, BATCH-SAVING RUNNER (drop-in) =================\n",
        "import os, json, time, csv\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- Where we commit progress (inside OUTPUT_DIR) ---\n",
        "TXT_DIR     = Path(OUTPUT_DIR) / \"txt\"\n",
        "LOG_CSV     = Path(OUTPUT_DIR) / \"batch_log.csv\"\n",
        "STATE_JSON  = Path(OUTPUT_DIR) / \"state.json\"\n",
        "BATCH_SIZE  = 50   # commit every N files (tweak to 25 if your session is flaky)\n",
        "\n",
        "TXT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _expected_txt_for_image(image_path: str) -> Path:\n",
        "    \"\"\"Mirror process_one() logic to compute the txt output path for an image.\"\"\"\n",
        "    rel  = os.path.relpath(image_path, INPUT_DIR)     # keeps subfolders\n",
        "    stem = Path(rel).with_suffix(\"\")                  # drop extension(s)\n",
        "    return Path(OUTPUT_DIR) / \"txt\" / f\"{stem}.txt\"\n",
        "\n",
        "def _append_rows_csv(csv_path: Path, rows: list, header: list):\n",
        "    file_exists = csv_path.exists()\n",
        "    csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f)\n",
        "        if not file_exists:\n",
        "            w.writerow(header)\n",
        "        w.writerows(rows)\n",
        "        f.flush()\n",
        "        os.fsync(f.fileno())\n",
        "\n",
        "def _load_state(state_path: Path):\n",
        "    if state_path.exists():\n",
        "        with open(state_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "    return {\"processed_files\": []}\n",
        "\n",
        "def _save_state(state_path: Path, state: dict):\n",
        "    tmp = state_path.with_suffix(\".json.tmp\")\n",
        "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(state, f, ensure_ascii=False, indent=2)\n",
        "        f.flush()\n",
        "        os.fsync(f.fileno())\n",
        "    tmp.replace(state_path)\n",
        "\n",
        "def run_resumable_batches():\n",
        "    # Discover inputs (reuses your list_images + config flags)\n",
        "    images = list_images(INPUT_DIR, RECURSIVE)\n",
        "    print(f\"Found {len(images)} images under {INPUT_DIR}\")\n",
        "\n",
        "    # Load state and build skip set (also skip if TXT already exists)\n",
        "    state = _load_state(STATE_JSON)\n",
        "    already = set(state.get(\"processed_files\", []))\n",
        "\n",
        "    to_do = []\n",
        "    for img in images:\n",
        "        txt_out = _expected_txt_for_image(img)\n",
        "        if txt_out.exists() or (img in already):\n",
        "            continue\n",
        "        to_do.append(img)\n",
        "\n",
        "    print(f\"Already processed (state or files): {len(images) - len(to_do)} | Remaining: {len(to_do)}\")\n",
        "\n",
        "    header = [\"timestamp_utc\",\"file\",\"txt_out\",\"chars\",\"lines\",\"status\",\"msg\",\"duration_s\"]\n",
        "    buffer_rows = []\n",
        "    t_batch = time.time()\n",
        "\n",
        "    for i, img in enumerate(tqdm(to_do, desc=\"OCR (resumable)\")):\n",
        "        t0 = time.time()\n",
        "        ts = datetime.utcnow().isoformat(timespec=\"seconds\")\n",
        "        txt_out = _expected_txt_for_image(img)\n",
        "\n",
        "        # Ensure subfolder exists (mirrors your process_one behavior)\n",
        "        txt_out.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            # process_one() ALREADY writes the TXT + JSON for this image\n",
        "            res = process_one(img, out_dir=OUTPUT_DIR, save_debug=SAVE_DEBUG_VIS)\n",
        "\n",
        "            # Quick stats for log\n",
        "            n_chars = res.get(\"n_chars\", 0)\n",
        "            n_lines = 1 + n_chars and str(res.get(\"text_path\",\"\")).count(\"\\n\")  # best-effort\n",
        "            buffer_rows.append([ts, img, str(txt_out), n_chars, n_lines, \"OK\", \"\", round(time.time()-t0, 3)])\n",
        "\n",
        "            # Mark state\n",
        "            already.add(img)\n",
        "            state[\"processed_files\"] = list(already)\n",
        "\n",
        "        except Exception as e:\n",
        "            buffer_rows.append([ts, img, str(txt_out), 0, 0, \"ERROR\", f\"{type(e).__name__}: {e}\", round(time.time()-t0, 3)])\n",
        "\n",
        "        # Commit every BATCH_SIZE or at the very end\n",
        "        if (len(buffer_rows) >= BATCH_SIZE) or (i == len(to_do) - 1):\n",
        "            _append_rows_csv(LOG_CSV, buffer_rows, header)\n",
        "            _save_state(STATE_JSON, state)\n",
        "            buffer_rows = []\n",
        "            # small sleep helps Drive indexing\n",
        "            time.sleep(0.4)\n",
        "\n",
        "    print(f\"Done chunk. Elapsed ~{round(time.time()-t_batch,1)}s. You can safely re-run to resume.\")\n"
      ],
      "metadata": {
        "id": "4_H_mZf1601n"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aOf4jwf_Q94O"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# 8) QUICK TEST (OPTIONAL): download one sample cover and run\n",
        "# =====================\n",
        "TEST_ONE = False\n",
        "if TEST_ONE:\n",
        "    import urllib.request\n",
        "    url = \"https://tapas.clarin.com/tapa/2025/08/06/20250806_thumb.jpg\"\n",
        "    Path('/content/sample_data').mkdir(exist_ok=True, parents=True)\n",
        "    local = \"/content/sample_data/clarin_20250806.jpg\"\n",
        "    urllib.request.urlretrieve(url, local)\n",
        "    INPUT_DIR = \"/content/sample_data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "ceec679bdc1740788e00c91a059b99d8",
            "bff16a6a61ac4f848f133fad0acf0521",
            "34b7bb7879bf4f9eadfeb4b95fedc88b",
            "de5b83f317e8405ba1a6aa733ede8e58",
            "f7e5cca843554950b73535db14b61c19",
            "2b6d93ddfa7347d78f8612dc583271d0",
            "5370bb2510e24103af5a6e717d69a906",
            "606543d93eec45de9c80294e8ab771bc",
            "26e85e3bd4874fa7b0552a023a08f9d6",
            "30249a7cba2043cf9fac509f9066de38",
            "791b3263b3264986b1254f7ce18ecc8f"
          ]
        },
        "id": "ltARVoRmRCPK",
        "outputId": "403e3df3-d22b-444e-e253-347bdd865b55"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 654 images under /content/drive/MyDrive/Data Justicialista/Clarin Cover Sentiment Analysis/Covers\n",
            "Already processed (state or files): 354 | Remaining: 300\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ceec679bdc1740788e00c91a059b99d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "OCR (resumable):   0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done chunk. Elapsed ~2924.4s. You can safely re-run to resume.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# =====================\n",
        "# 9) GO — run the batch\n",
        "# =====================\n",
        "run_resumable_batches()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ceec679bdc1740788e00c91a059b99d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bff16a6a61ac4f848f133fad0acf0521",
              "IPY_MODEL_34b7bb7879bf4f9eadfeb4b95fedc88b",
              "IPY_MODEL_de5b83f317e8405ba1a6aa733ede8e58"
            ],
            "layout": "IPY_MODEL_f7e5cca843554950b73535db14b61c19"
          }
        },
        "bff16a6a61ac4f848f133fad0acf0521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b6d93ddfa7347d78f8612dc583271d0",
            "placeholder": "​",
            "style": "IPY_MODEL_5370bb2510e24103af5a6e717d69a906",
            "value": "OCR (resumable): 100%"
          }
        },
        "34b7bb7879bf4f9eadfeb4b95fedc88b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_606543d93eec45de9c80294e8ab771bc",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26e85e3bd4874fa7b0552a023a08f9d6",
            "value": 300
          }
        },
        "de5b83f317e8405ba1a6aa733ede8e58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30249a7cba2043cf9fac509f9066de38",
            "placeholder": "​",
            "style": "IPY_MODEL_791b3263b3264986b1254f7ce18ecc8f",
            "value": " 300/300 [48:44&lt;00:00,  9.95s/it]"
          }
        },
        "f7e5cca843554950b73535db14b61c19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b6d93ddfa7347d78f8612dc583271d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5370bb2510e24103af5a6e717d69a906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "606543d93eec45de9c80294e8ab771bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26e85e3bd4874fa7b0552a023a08f9d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "30249a7cba2043cf9fac509f9066de38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "791b3263b3264986b1254f7ce18ecc8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}