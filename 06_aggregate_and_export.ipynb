{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5u5fuA8spqcWh6dAxXB70",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renzungo/Clarin_Covers_Sent_Analysis/blob/sentiment/06_aggregate_and_export.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSCNs-Fh2zLl",
        "outputId": "ce231eb0-d81f-4929-e6b1-423e8374f9e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Wrote: /content/drive/MyDrive/Data Justicialista/Clarin Cover Sentiment Analysis/odc_analytics_out/covers_master.csv\n",
            "Also wrote:\n",
            "/content/drive/MyDrive/Data Justicialista/Clarin Cover Sentiment Analysis/odc_analytics_out/timeseries_sentiment.csv\n",
            "/content/drive/MyDrive/Data Justicialista/Clarin Cover Sentiment Analysis/odc_analytics_out/timeseries_topics.csv\n",
            "/content/drive/MyDrive/Data Justicialista/Clarin Cover Sentiment Analysis/odc_analytics_out/timeseries_people.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive; drive.mount('/content/drive', force_remount=True)\n",
        "!pip -q install pandas numpy\n",
        "\n",
        "import os, json, pandas as pd, numpy as np\n",
        "\n",
        "WORK_DIR = \"/content/drive/MyDrive/Data Justicialista/Clarin Cover Sentiment Analysis/odc_pipeline_work\"\n",
        "OUT_DIR  = \"/content/drive/MyDrive/Data Justicialista/Clarin Cover Sentiment Analysis/odc_analytics_out\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "def exists(p):\n",
        "    return os.path.exists(p) and os.path.getsize(p) > 0\n",
        "\n",
        "# ---- Base texts (needed for the file/date backbone)\n",
        "base_path_parquet = os.path.join(WORK_DIR, \"base.parquet\")\n",
        "if not exists(base_path_parquet):\n",
        "    raise RuntimeError(\"base.parquet not found. Run notebook 01 first.\")\n",
        "df_base = pd.read_parquet(base_path_parquet)[[\"file\",\"date\"]]\n",
        "\n",
        "# ---- Topics (parquet or csv rows)\n",
        "topics_parquet = os.path.join(WORK_DIR, \"topics.parquet\")\n",
        "topics_rows_csv = os.path.join(WORK_DIR, \"topics_rows.csv\")  # optional streaming format if you switch 02 to rows\n",
        "\n",
        "if exists(topics_parquet):\n",
        "    df_topics = pd.read_parquet(topics_parquet)[[\"file\",\"date\",\"topics_json\",\"top_topic\",\"top_topic_score\"]]\n",
        "elif exists(topics_rows_csv):\n",
        "    df_topics = pd.read_csv(topics_rows_csv)[[\"file\",\"date\",\"topics_json\",\"top_topic\",\"top_topic_score\"]]\n",
        "else:\n",
        "    df_topics = pd.DataFrame(columns=[\"file\",\"date\",\"topics_json\",\"top_topic\",\"top_topic_score\"])\n",
        "\n",
        "# ---- Sentiment overall / eco-gov (parquet or csv rows)\n",
        "sent_parquet = os.path.join(WORK_DIR, \"sentiment.parquet\")\n",
        "sent_rows_csv = os.path.join(WORK_DIR, \"sentiment_rows.csv\")  # optional streaming format if you switch 03 to rows\n",
        "\n",
        "if exists(sent_parquet):\n",
        "    df_sent = pd.read_parquet(sent_parquet)[[\"file\",\"date\",\"overall_sentiment\",\"overall_score\",\"eco_share\",\"eco_sentiment\",\"eco_sent_score\",\"gov_share\",\"gov_sentiment\",\"gov_sent_score\"]]\n",
        "elif exists(sent_rows_csv):\n",
        "    df_sent = pd.read_csv(sent_rows_csv)[[\"file\",\"date\",\"overall_sentiment\",\"overall_score\",\"eco_share\",\"eco_sentiment\",\"eco_sent_score\",\"gov_share\",\"gov_sentiment\",\"gov_sent_score\"]]\n",
        "else:\n",
        "    df_sent = pd.DataFrame(columns=[\"file\",\"date\",\"overall_sentiment\",\"overall_score\",\"eco_share\",\"eco_sentiment\",\"eco_sent_score\",\"gov_share\",\"gov_sentiment\",\"gov_sent_score\"])\n",
        "\n",
        "# ---- NER counts (from step 04)\n",
        "ner_counts_parquet = os.path.join(WORK_DIR, \"ner_counts.parquet\")\n",
        "if exists(ner_counts_parquet):\n",
        "    df_counts = pd.read_parquet(ner_counts_parquet)[[\"file\",\"date\",\"people_json\",\"orgs_json\",\"places_json\"]]\n",
        "else:\n",
        "    df_counts = pd.DataFrame(columns=[\"file\",\"date\",\"people_json\",\"orgs_json\",\"places_json\"])\n",
        "\n",
        "# ---- Lexicon top words (from 01)\n",
        "lex_parquet = os.path.join(WORK_DIR, \"lexicon.parquet\")\n",
        "if exists(lex_parquet):\n",
        "    df_lex = pd.read_parquet(lex_parquet)[[\"file\",\"date\",\"top_pos_words_json\",\"top_neg_words_json\"]]\n",
        "else:\n",
        "    df_lex = pd.DataFrame(columns=[\"file\",\"date\",\"top_pos_words_json\",\"top_neg_words_json\"])\n",
        "\n",
        "# ---- Entities (growing CSV from streaming 05)\n",
        "entities_csv = os.path.join(OUT_DIR, \"entities_sentiment.csv\")\n",
        "if exists(entities_csv):\n",
        "    df_entities = pd.read_csv(entities_csv)\n",
        "else:\n",
        "    df_entities = pd.DataFrame(columns=[\"file\",\"date\",\"entity_norm\",\"entity_type\",\"mentions\",\"avg_score\",\"pos_share\",\"neu_share\",\"neg_share\"])\n",
        "\n",
        "# ---- N-grams\n",
        "ngrams_csv = os.path.join(OUT_DIR, \"ngrams.csv\")\n",
        "if exists(ngrams_csv):\n",
        "    df_ngrams = pd.read_csv(ngrams_csv)\n",
        "else:\n",
        "    df_ngrams = pd.DataFrame(columns=[\"file\",\"date\",\"ngram\",\"n\",\"count\"])\n",
        "\n",
        "# ---- Build master (left-join whatever we have)\n",
        "df_master = (df_base\n",
        "             .merge(df_sent,   on=[\"file\",\"date\"], how=\"left\")\n",
        "             .merge(df_topics, on=[\"file\",\"date\"], how=\"left\")\n",
        "             .merge(df_counts, on=[\"file\",\"date\"], how=\"left\")\n",
        "             .merge(df_lex,    on=[\"file\",\"date\"], how=\"left\"))\n",
        "\n",
        "# Ensure columns exist\n",
        "for col in [\"topics_json\",\"people_json\",\"orgs_json\",\"places_json\",\"top_pos_words_json\",\"top_neg_words_json\",\n",
        "            \"overall_sentiment\",\"overall_score\",\"eco_share\",\"eco_sentiment\",\"eco_sent_score\",\"gov_share\",\"gov_sentiment\",\"gov_sent_score\"]:\n",
        "    if col not in df_master.columns: df_master[col] = np.nan\n",
        "\n",
        "master_csv = os.path.join(OUT_DIR, \"covers_master.csv\")\n",
        "df_master.to_csv(master_csv, index=False, encoding=\"utf-8\")\n",
        "print(\"Wrote:\", master_csv)\n",
        "\n",
        "# ---- Timeseries: sentiment by date (based on whatever has overall_sentiment)\n",
        "def mean_share(series, target):\n",
        "    if series.empty: return np.nan\n",
        "    vals = [1 if x==target else 0 for x in series]\n",
        "    return np.mean(vals) if len(vals) else np.nan\n",
        "\n",
        "ts_sent = (df_master.dropna(subset=[\"date\"])\n",
        "           .groupby(\"date\", as_index=False)\n",
        "           .agg(overall_pos=(\"overall_sentiment\", lambda s: mean_share(s, \"POS\")),\n",
        "                overall_neu=(\"overall_sentiment\", lambda s: mean_share(s, \"NEU\")),\n",
        "                overall_neg=(\"overall_sentiment\", lambda s: mean_share(s, \"NEG\")),\n",
        "                eco_share=(\"eco_share\",\"mean\"),\n",
        "                gov_share=(\"gov_share\",\"mean\")))\n",
        "\n",
        "ts_sent_csv = os.path.join(OUT_DIR, \"timeseries_sentiment.csv\")\n",
        "ts_sent.to_csv(ts_sent_csv, index=False, encoding=\"utf-8\")\n",
        "\n",
        "# ---- Topic timeseries (if any topics exist yet)\n",
        "def explode_topics(df):\n",
        "    rows = []\n",
        "    for r in df.itertuples(index=False):\n",
        "        try:\n",
        "            topics = json.loads(r.topics_json) if pd.notna(r.topics_json) else {}\n",
        "        except Exception:\n",
        "            topics = {}\n",
        "        for k,v in topics.items():\n",
        "            rows.append({\"date\": r.date, \"file\": r.file, \"topic\": k, \"score\": v})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df_topics_ex = explode_topics(df_master)\n",
        "if df_topics_ex.empty:\n",
        "    topic_ts = pd.DataFrame(columns=[\"date\",\"topic\",\"avg_topic_score\",\"covers\"])\n",
        "else:\n",
        "    topic_ts = (df_topics_ex.dropna(subset=[\"date\"])\n",
        "                .groupby([\"date\",\"topic\"], as_index=False)\n",
        "                .agg(avg_topic_score=(\"score\",\"mean\"),\n",
        "                     covers=(\"file\",\"count\")))\n",
        "\n",
        "topic_ts_csv = os.path.join(OUT_DIR, \"timeseries_topics.csv\")\n",
        "topic_ts.to_csv(topic_ts_csv, index=False, encoding=\"utf-8\")\n",
        "\n",
        "# ---- People timeseries (works with partial entities)\n",
        "if not df_entities.empty:\n",
        "    people_ts = (df_entities[df_entities[\"entity_type\"]==\"PER\"]\n",
        "                 .dropna(subset=[\"date\"])\n",
        "                 .groupby([\"date\",\"entity_norm\"], as_index=False)\n",
        "                 .agg(mentions=(\"mentions\",\"sum\"),\n",
        "                      pos_share=(\"pos_share\",\"mean\"),\n",
        "                      neg_share=(\"neg_share\",\"mean\")))\n",
        "else:\n",
        "    people_ts = pd.DataFrame(columns=[\"date\",\"entity_norm\",\"mentions\",\"pos_share\",\"neg_share\"])\n",
        "\n",
        "people_ts_csv = os.path.join(OUT_DIR, \"timeseries_people.csv\")\n",
        "people_ts.to_csv(people_ts_csv, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"Also wrote:\")\n",
        "print(ts_sent_csv)\n",
        "print(topic_ts_csv)\n",
        "print(people_ts_csv)\n"
      ]
    }
  ]
}